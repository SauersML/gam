name: Benchmark Suite

on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: "0 4 * * *"

env:
  CARGO_TERM_COLOR: always
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  PIP_NO_PYTHON_VERSION_WARNING: "1"
  R_LIBS_USER: ${{ github.workspace }}/.r-lib

jobs:
  prepare:
    name: Prepare benchmark matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Build scenario matrix
        id: matrix
        shell: bash
        run: |
          python3 - <<'PY'
          import json
          import os
          from pathlib import Path
          cfg = json.loads(Path("bench/scenarios.json").read_text())
          scenarios = cfg.get("scenarios", [])
          include = [{"scenario": s["name"]} for s in scenarios if "name" in s]
          if not include:
              raise SystemExit("No benchmark scenarios found in bench/scenarios.json")
          matrix = {"include": include}
          out = Path(os.environ["GITHUB_OUTPUT"])
          with out.open("a", encoding="utf-8") as fh:
              fh.write(f"matrix={json.dumps(matrix)}\n")
          print(json.dumps(matrix))
          PY

  bootstrap-runtime:
    name: Bootstrap benchmark runtime
    needs: prepare
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Rust stable
        uses: dtolnay/rust-toolchain@stable

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Set up R
        uses: r-lib/actions/setup-r@v2

      - name: Build shared runtime once
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p bench/runtime/pydeps bench/runtime/r-lib
          python3 -m pip install --upgrade pip setuptools wheel
          python3 -m pip install --target bench/runtime/pydeps numpy pandas pygam lifelines
          Rscript -e 'install.packages(c("mgcv","jsonlite"), repos="https://cloud.r-project.org", lib="bench/runtime/r-lib")'
          cargo build --release --bin gam
          cp target/release/gam bench/runtime/gam
          chmod +x bench/runtime/gam

      - name: Upload shared runtime artifact
        uses: actions/upload-artifact@v4
        with:
          name: bench-runtime
          path: bench/runtime
          if-no-files-found: error

  bench-shard:
    name: Bench ${{ matrix.scenario }}
    needs:
      - prepare
      - bootstrap-runtime
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      max-parallel: 8
      matrix: ${{ fromJSON(needs.prepare.outputs.matrix) }}
    env:
      OMP_NUM_THREADS: "1"
      OPENBLAS_NUM_THREADS: "1"
      MKL_NUM_THREADS: "1"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Set up R
        uses: r-lib/actions/setup-r@v2

      - name: Download shared runtime
        uses: actions/download-artifact@v4
        with:
          name: bench-runtime
          path: bench/runtime

      - name: Run scenario shard
        shell: bash
        env:
          PYTHONPATH: ${{ github.workspace }}/bench/runtime/pydeps
          R_LIBS_USER: ${{ github.workspace }}/bench/runtime/r-lib
          BENCH_GAM_BIN: ${{ github.workspace }}/bench/runtime/gam
        run: |
          set -euo pipefail
          echo "== Benchmark shard diagnostics =="
          echo "pwd: $(pwd)"
          echo "scenario: ${{ matrix.scenario }}"
          echo "python: $(command -v python3)"
          python3 --version
          echo "Rscript: $(command -v Rscript)"
          Rscript --version
          echo "BENCH_GAM_BIN=${BENCH_GAM_BIN}"
          echo "PYTHONPATH=${PYTHONPATH}"
          echo "R_LIBS_USER=${R_LIBS_USER}"
          echo "bench/runtime tree:"
          ls -la bench/runtime || true
          ls -la bench/runtime/pydeps | sed -n '1,40p' || true
          ls -la bench/runtime/r-lib | sed -n '1,40p' || true
          echo "gam binary before chmod:"
          ls -l "${BENCH_GAM_BIN}" || true
          file "${BENCH_GAM_BIN}" || true
          chmod +x "${BENCH_GAM_BIN}" || true
          echo "gam binary after chmod:"
          ls -l "${BENCH_GAM_BIN}" || true
          file "${BENCH_GAM_BIN}" || true
          test -x "${BENCH_GAM_BIN}"
          echo "smoke-import python deps"
          python3 - <<'PY'
          import numpy, pandas, pygam, lifelines
          print("python deps ok")
          PY
          echo "smoke-load R deps"
          Rscript -e 'library(mgcv); library(jsonlite); cat("R deps ok\n")'
          mkdir -p bench/results-shards
          echo "running benchmark shard..."
          python3 bench/run_suite.py \
            --scenarios bench/scenarios.json \
            --scenario-name "${{ matrix.scenario }}" \
            --out "bench/results-shards/${{ matrix.scenario }}.json"
          echo "shard output:"
          ls -l "bench/results-shards/${{ matrix.scenario }}.json"

      - name: Upload shard artifact
        uses: actions/upload-artifact@v4
        with:
          name: bench-${{ matrix.scenario }}
          path: bench/results-shards/${{ matrix.scenario }}.json
          if-no-files-found: error

  aggregate:
    name: Aggregate benchmark results
    needs: bench-shard
    runs-on: ubuntu-latest
    steps:
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: bench/artifacts

      - name: Merge shard JSON outputs
        shell: bash
        run: |
          python3 - <<'PY'
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path

          def fmt_num(v, digits=4):
              if v is None:
                  return "—"
              try:
                  return f"{float(v):.{digits}f}"
              except Exception:
                  return "—"

          def fmt_status(row):
              status = str(row.get("status", "unknown"))
              if status == "ok":
                  return "ok"
              return f"failed: {row.get('error', 'unknown error')}"

          def md_escape(v):
              if v is None:
                  return "—"
              s = str(v).replace("\n", " ").strip()
              return s.replace("|", "\\|")

          artifacts = Path("bench/artifacts")
          files = sorted(artifacts.rglob("*.json"))
          if not files:
              raise SystemExit("No shard artifacts found to merge.")

          merged = []
          for fp in files:
              payload = json.loads(fp.read_text())
              merged.extend(payload.get("results", []))

          merged_sorted = sorted(
              merged,
              key=lambda r: (
                  str(r.get("scenario_name", "")),
                  str(r.get("contender", "")),
              ),
          )

          out = Path("bench/results.nightly.json")
          out.write_text(
              json.dumps(
                  {
                      "created_at_utc": datetime.now(timezone.utc).isoformat(),
                      "results": merged,
                  },
                  indent=2,
              )
          )
          print(f"Wrote {out} from {len(files)} shard files with {len(merged)} rows.")

          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          if summary_path:
              ok_count = sum(1 for r in merged_sorted if str(r.get("status", "")) == "ok")
              fail_count = len(merged_sorted) - ok_count
              family_labels = {"binomial": "Classification", "gaussian": "Regression", "survival": "Survival"}
              family_rows = {}
              for r in merged_sorted:
                  fam = str(r.get("family") or "unknown")
                  family_rows.setdefault(fam, []).append(r)

              def primary_metric(row):
                  fam = str(row.get("family") or "")
                  if fam == "binomial":
                      return "Brier", row.get("brier"), "min"
                  if fam == "gaussian":
                      return "RMSE", row.get("rmse"), "min"
                  if fam == "survival":
                      return "C-index", row.get("auc"), "max"
                  return "n/a", None, "min"

              def winner_for_family(fam):
                  rows = [r for r in family_rows.get(fam, []) if str(r.get("status", "")) == "ok"]
                  if not rows:
                      return None
                  vals = []
                  for r in rows:
                      _, v, mode = primary_metric(r)
                      if v is None:
                          continue
                      vals.append((float(v), r, mode))
                  if not vals:
                      return None
                  mode = vals[0][2]
                  return (min(vals, key=lambda x: x[0]) if mode == "min" else max(vals, key=lambda x: x[0]))[1]

              lines = []
              lines.append("## Benchmark Results")
              lines.append("")
              lines.append(f"- Generated: `{datetime.now(timezone.utc).isoformat()}`")
              lines.append(f"- Shards merged: `{len(files)}`")
              lines.append(f"- Result rows: `{len(merged_sorted)}` (`{ok_count}` ok, `{fail_count}` failed)")
              lines.append("- Primary metrics by task:")
              lines.append("  - Classification: lower Brier (with AUC + LogLoss shown)")
              lines.append("  - Regression: lower RMSE (with MAE + R2 shown)")
              lines.append("  - Survival: higher C-index")
              lines.append("")
              lines.append("### Leaders")
              lines.append("")
              lines.append("| Task | Winner | Primary Metric | Value |")
              lines.append("|---|---|---|---:|")
              for fam in ("binomial", "gaussian", "survival"):
                  winner = winner_for_family(fam)
                  if winner is None:
                      lines.append(f"| {family_labels.get(fam, fam)} | — | — | — |")
                      continue
                  m_name, m_val, _ = primary_metric(winner)
                  lines.append(
                      f"| {family_labels.get(fam, fam)} | "
                      f"{md_escape(winner.get('contender'))} ({md_escape(winner.get('scenario_name'))}) | "
                      f"{m_name} | {fmt_num(m_val)} |"
                  )
              lines.append("")
              lines.append("### Full Table")
              lines.append("")
              lines.append("| Task | Scenario | Contender | Status | C-index/AUC | Brier | LogLoss | RMSE | MAE | R2 | Fit (s) | Predict (s) |")
              lines.append("|---|---|---|---|---:|---:|---:|---:|---:|---:|---:|---:|")
              for r in merged_sorted:
                  fam = str(r.get("family") or "unknown")
                  lines.append(
                      "| "
                      + " | ".join(
                          [
                              md_escape(family_labels.get(fam, fam)),
                              md_escape(r.get("scenario_name")),
                              md_escape(r.get("contender")),
                              md_escape(fmt_status(r)),
                              fmt_num(r.get("auc")),
                              fmt_num(r.get("brier")),
                              fmt_num(r.get("logloss")),
                              fmt_num(r.get("rmse")),
                              fmt_num(r.get("mae")),
                              fmt_num(r.get("r2")),
                              fmt_num(r.get("fit_sec")),
                              fmt_num(r.get("predict_sec")),
                          ]
                      )
                      + " |"
                  )
              lines.append("")
              lines.append("### Model Specs")
              lines.append("")
              lines.append("| Scenario | Contender | Model Spec |")
              lines.append("|---|---|---|")
              for r in merged_sorted:
                  lines.append(
                      f"| {md_escape(r.get('scenario_name'))} | {md_escape(r.get('contender'))} | {md_escape(r.get('model_spec'))} |"
                  )
              Path(summary_path).write_text("\n".join(lines) + "\n")
          PY

      - name: Upload merged nightly results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-nightly
          path: bench/results.nightly.json
          if-no-files-found: error
